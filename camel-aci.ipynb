{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872981be",
   "metadata": {},
   "source": [
    "# CAMEL Cookbook - Object Detection with ACI.dev MCP Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c98d9",
   "metadata": {},
   "source": [
    "**Description:** Learn how to build an object detection agent using CAMEL AI and ACI.dev's MCP protocol for seamless ML tasks. \n",
    "\n",
    "‚≠ê Star us on [GitHub](https://github.com/camel-ai/camel), join our [Discord](https://discord.gg/EXAMPLE), or follow us on [X](https://x.com/camelaiorg)\n",
    "\n",
    "This cookbook shows how to build a powerful object detection agent using CAMEL AI connected to ACI.dev's MCP tools. We'll create an agent that analyzes images, detects objects like cars or trees, and explains results in natural language‚Äîall without writing complex ML code.\n",
    "\n",
    "**Key Learnings:**\n",
    "- Why agents need tools to be truly useful.\n",
    "- How MCP enables dynamic, aware tool usage for tasks like object detection.\n",
    "- Setting up CAMEL with ACI.dev for real-time image analysis.\n",
    "- Building and running your own object detection agent.\n",
    "- Handling outputs with summaries, tables, and visualized results.\n",
    "\n",
    "This setup uses CAMEL's `MCPToolkit` to connect to ACI.dev's MCP servers, powering object detection via Replicate's ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad4834",
   "metadata": {},
   "source": [
    "### üì¶ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install camel-ai aci-mcp aci-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e21e0",
   "metadata": {},
   "source": [
    "Set up keys (ACI_API_KEY, LINKED_ACCOUNT_OWNER_ID, GOOGLE_API_KEY, REPLICATE_API_TOKEN) in `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e484b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf512d1",
   "metadata": {},
   "source": [
    "### Define CAMEL agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8125d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"ObjectDetectionAgent\"\n",
    "system_prompt=\"\"\"\n",
    "You are a specialized Object Detection Agent. Your primary function is to use the `REPLICATE.run` tool for object detection and present the findings in a user-friendly format. \"\n",
    "\"The user will provide a text prompt containing an image URL and a query. You must extract the `image` URL and the `query` object(s). \"\n",
    "\"Immediately call the `REPLICATE.run` tool. The `input` must be a dictionary with two keys: `image` (the URL) and `query` (a string of the object(s)). \"\n",
    "\"Do not ask for clarification; make a reasonable inference if the query is ambiguous. \"\n",
    "\"After receiving the tool's output, format your response as follows: \"\n",
    "\"- **Natural Language Summary:** Start with a detailed friendly, insightful analysis of the detection results in plain English. \"\n",
    "\"- **Markdown Table:** Create a markdown table with columns: 'Object', 'Confidence Score', and 'Bounding Box Coordinates'. \"\n",
    "\"- **Result Image:** If the tool provides a URL for an image with bounding boxes, display it using markdown: `![Detected Objects](URL_HERE)`. \"\n",
    "\"Whenever I give you a link, trigger the tool call, extract its outputs and links, and present me in a proper markdown format with detailed analysis from the tool call in natural language.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de159dc",
   "metadata": {},
   "source": [
    "### MCP servers configuration using ACI.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b360b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<camel.toolkits.mcp_toolkit.MCPToolkit at 0x7251601faf90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from camel.toolkits import MCPToolkit\n",
    "mcp_config = {\n",
    "    \"mcpServers\": {\n",
    "        \"aci_apps\": {\n",
    "            \"command\": \"aci-mcp\",\n",
    "            \"args\": [\n",
    "                \"apps-server\",\n",
    "                \"--apps=REPLICATE\",\n",
    "                \"--linked-account-owner-id\",\n",
    "                \"parthshr370\"\n",
    "            ],\n",
    "            \"env\": {\"ACI_API_KEY\": os.getenv(\"ACI_API_KEY\")},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "mcp_toolkit = MCPToolkit(config_dict=mcp_config)\n",
    "await mcp_toolkit.connect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f65a18",
   "metadata": {},
   "source": [
    "Define the CAMEL agent with GEMINI 2.5 Flash model with Replicate MCP tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "834b52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.messages import BaseMessage\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType\n",
    "\n",
    "tools = mcp_toolkit.get_tools()\n",
    "\n",
    "# Initialize Gemini model\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.GEMINI,\n",
    "    model_type=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    model_config_dict={\"temperature\": 0.0, \"max_tokens\": 4096},\n",
    ")\n",
    "\n",
    "# Create system message\n",
    "sys_msg = BaseMessage.make_assistant_message(\n",
    "    role_name=agent_name,\n",
    "    content=system_prompt,\n",
    ")\n",
    "\n",
    "agent = ChatAgent(model=model, system_message=sys_msg, tools=tools, memory=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c9a64",
   "metadata": {},
   "source": [
    "After create the agent, let's do some examples using Replicate to identify the following images:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304b8ff",
   "metadata": {},
   "source": [
    "![](https://images.pexels.com/photos/2255935/pexels-photo-2255935.jpeg)\n",
    "![](https://www.livemint.com/rf/Image-621x414/LiveMint/Period1/2012/10/01/Photos/Road621.jpg)\n",
    "![](https://media.business-humanrights.org/media/images/16278498935_dac4d8f223_o.2e16d0ba.fill-1000x1000-c50.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51551c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an analysis of the detected produce in the vegetable stall image:\n",
      "\n",
      "**Natural Language Summary:**\n",
      "The object detection model successfully identified several types of produce from your query. We found multiple instances of **onions**, **cabbage**, **tomatoes**, and **carrots** with varying confidence levels. A single **zucchini** and a **cucumber** were also detected. Interestingly, a **beet** was identified, though with a lower confidence score. The model seems to have accurately pinpointed the locations of these vegetables within the image, providing bounding box coordinates for each detection.\n",
      "\n",
      "**Detection Results:**\n",
      "\n",
      "| Object | Confidence Score | Bounding Box Coordinates (y_min, x_min, y_max, x_max) |\n",
      "|---|---|---|\n",
      "| carrot | 0.465 | [294, 916, 1001, 1438] |\n",
      "| cabbage | 0.497 | [110, 1982, 1436, 3150] |\n",
      "| onion | 0.412 | [742, 3535, 1378, 4186] |\n",
      "| onion | 0.423 | [778, 4032, 1507, 4837] |\n",
      "| onion | 0.373 | [10, 3733, 785, 4501] |\n",
      "| cabbage | 0.328 | [6, 3088, 1022, 3850] |\n",
      "| tomato | 0.409 | [1214, 2242, 3445, 3768] |\n",
      "| tomato | 0.327 | [1913, 2846, 2537, 3407] |\n",
      "| onion | 0.321 | [4, 4434, 568, 5173] |\n",
      "| tomato | 0.292 | [1782, 4008, 2517, 4669] |\n",
      "| tomato | 0.254 | [1503, 4515, 2212, 5172] |\n",
      "| zucchini | 0.445 | [727, 1767, 1538, 2331] |\n",
      "| beet | 0.263 | [1039, 433, 1360, 687] |\n",
      "| tomato | 0.288 | [1662, 2397, 2211, 2920] |\n",
      "| tomato | 0.331 | [1501, 3778, 3446, 5172] |\n",
      "| onion | 0.259 | [725, 4798, 1447, 5176] |\n",
      "| cucumber | 0.312 | [213, 1482, 1066, 1785] |\n",
      "| carrot | 0.313 | [296, 1079, 951, 1422] |\n",
      "| onion | 0.265 | [9, 3543, 1500, 5171] |\n",
      "\n",
      "**Result Image:**\n",
      "![Detected Objects](https://replicate.delivery/xezq/PiXypyoenjy0dShEFUrz18zh3hTl3U3K7ltPoM6c5BwxOXiKA/result.png)\n",
      "\n",
      "Here's an analysis of the detected objects in the busy street scene:\n",
      "\n",
      "**Natural Language Summary:**\n",
      "The object detection model successfully identified several elements in the busy street scene. Numerous **cars** were detected across the image, indicating a high volume of traffic. We also found instances of **trucks**, suggesting commercial vehicles are present. Additionally, the model identified a few instances of **people**, though their confidence scores are generally lower, likely due to their smaller size or partial visibility in the scene. Interestingly, one detection was labeled as both a \"truck\" and a \"bus\" with different confidence scores, which might indicate some ambiguity for the model in distinguishing between larger vehicles in certain contexts.\n",
      "\n",
      "**Detection Results:**\n",
      "\n",
      "| Object | Confidence Score | Bounding Box Coordinates (y_min, x_min, y_max, x_max) |\n",
      "|---|---|---|\n",
      "| car | 0.400 | [364, 280, 507, 404] |\n",
      "| car | 0.423 | [1, 248, 155, 412] |\n",
      "| car | 0.397 | [200, 260, 333, 395] |\n",
      "| car | 0.405 | [362, 111, 465, 195] |\n",
      "| car | 0.359 | [60, 74, 150, 151] |\n",
      "| car | 0.334 | [267, 60, 360, 149] |\n",
      "| truck | 0.403 | [193, 128, 290, 229] |\n",
      "| car | 0.305 | [305, 210, 436, 317] |\n",
      "| car | 0.289 | [184, 64, 262, 132] |\n",
      "| people | 0.286 | [227, 281, 270, 315] |\n",
      "| people | 0.298 | [557, 301, 601, 337] |\n",
      "| car | 0.274 | [529, 285, 619, 409] |\n",
      "| car | 0.268 | [5, 1, 613, 409] |\n",
      "| bus | 0.252 | [193, 128, 290, 229] |\n",
      "| truck | 0.347 | [83, 162, 177, 271] |\n",
      "\n",
      "**Result Image:**\n",
      "![Detected Objects](https://replicate.delivery/xezq/yKsqf8ft1XoZt01zvqKKnyZPl2Hq9eVuec41yLB4QVqz25SUB/result.png)\n",
      "2025-07-26 20:07:17,215 - camel.agents.chat_agent - WARNING - Message with 586 tokens exceeds remaining budget of 493. Slicing into smaller chunks.\n",
      "2025-07-26 20:07:17,226 - camel.camel.memories.context_creators.score_based - WARNING - Context truncation performed: before=4485, after=3426, limit=4096\n",
      "2025-07-26 20:07:20,586 - camel.camel.memories.context_creators.score_based - WARNING - Context truncation performed: before=4485, after=3426, limit=4096\n",
      "Here's an analysis of the detected objects in the warehouse scene:\n",
      "\n",
      "**Natural Language Summary:**\n",
      "The object detection model successfully identified several key elements within the warehouse image. It detected multiple **persons** with high confidence, indicating their presence and activity in the scene. Several **cardboard boxes** were also identified, some with higher confidence than others, suggesting varying visibility or clarity. A **conveyor belt** was detected, which is crucial for understanding the operational aspect of the warehouse. The model's ability to pinpoint these specific objects provides a good overview of the scene's contents.\n",
      "\n",
      "**Detection Results:**\n",
      "\n",
      "| Object | Confidence Score | Bounding Box Coordinates (y_min, x_min, y_max, x_max) |\n",
      "|---|---|---|\n",
      "| persons | 0.788 | [4, 286, 93, 501] |\n",
      "| persons | 0.752 | [71, 417, 207, 556] |\n",
      "| cardboard boxes | 0.461 | [158, 256, 500, 489] |\n",
      "| cardboard boxes | 0.284 | [367, 170, 450, 237] |\n",
      "| conveyor belts | 0.292 | [274, 216, 361, 285] |\n",
      "| cardboard boxes | 0.270 | [0, 74, 556, 555] |\n",
      "| cardboard boxes | 0.281 | [301, 293, 499, 479] |\n",
      "\n",
      "**Result Image:**\n",
      "![Detected Objects](https://replicate.delivery/xezq/cc59yfbCNfvZWkFeVewkHW77kqVdwOBGMqUgRFIbmVeiuzloC/result.png)\n"
     ]
    }
   ],
   "source": [
    "tasks = (\n",
    "    \"Analyze the vegetable stall and identify all produce, including tomato, onion, cabbage, cucumber, zucchini, carrot, and beet, in this image: https://images.pexels.com/photos/2255935/pexels-photo-2255935.jpeg\",\n",
    "    \"Analyze the busy street scene and identify all vehicles, such as car, bus, and truck, as well as people, in this image: https://www.livemint.com/rf/Image-621x414/LiveMint/Period1/2012/10/01/Photos/Road621.jpg\",\n",
    "    \"Analyze the warehouse scene and identify persons, cardboard boxes, and conveyor belts in this image: https://media.business-humanrights.org/media/images/16278498935_dac4d8f223_o.2e16d0ba.fill-1000x1000-c50.jpg\"\n",
    ")\n",
    "\n",
    "for task in tasks:\n",
    "    response = await agent.astep(task)\n",
    "    print(response.msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41f02d",
   "metadata": {},
   "source": [
    "Finally, we disconnect the MCP toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b7ddd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "await mcp_toolkit.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
